{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Self-Trained Model\n",
    "\n",
    "## Instructions\n",
    "Run below cells to initialise model and parameters.\n",
    "Run 'Actual Program Code' (labelled below) to run demo.\n",
    "\n",
    "## Requirements\n",
    "Ensure you have torch, torchvision, opencv, numpy, Pillow installed. You can pip install them.\n",
    "Also, git clone yolov5 at https://github.com/ultralytics/yolov5 into the same folder as this ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\junha\\\\OneDrive\\\\Documents\\\\SUTD\\\\Term 7\\\\50.035 - Computer Vision\\\\50.035CV-C01-Team10-Visual-Interactive-Game\\\\Hand_Detection_YOLOv5', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\python39.zip', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\DLLs', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39', '', 'C:\\\\Users\\\\junha\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\junha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\junha\\\\.ipython', '../', '../Hand_Pose_Estimation_2D']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Hand_Pose_Estimation_2D')\n",
    "print(sys.path)\n",
    "\n",
    "\n",
    "from Hand_Pose_Estimation_2D.Utils.model import ShallowUNet\n",
    "from Hand_Pose_Estimation_2D.Utils.utils import (\n",
    "    COLORMAP,\n",
    "    heatmaps_to_coordinates,\n",
    "    N_KEYPOINTS,\n",
    "    RAW_IMG_SIZE,\n",
    "    MODEL_IMG_SIZE,\n",
    "    show_batch_predictions,\n",
    "    DATASET_MEANS,\n",
    "    DATASET_STDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\junha/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-4-16 torch 1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "BEST_WEIGHT_PATH = '.\\\\weights\\\\best.pt'\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', BEST_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "UNET_PATH = '..\\\\Hand_Pose_Estimation_2D\\\\model_final'\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "unet_model = ShallowUNet(3,21)\n",
    "unet_model.load_state_dict(\n",
    "    torch.load(UNET_PATH, map_location=DEVICE)\n",
    ")\n",
    "unet_model.to(DEVICE)\n",
    "unet_model.eval()\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINT_LIST = [\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [0, 5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16],\n",
    "    [0, 17, 18, 19, 20],\n",
    "    [5, 9, 13, 17]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebCam Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(labels, cord, frame):\n",
    "        \"\"\"\n",
    "        Takes a frame and its results as input, and plots the bounding boxes and label on to the frame.\n",
    "        :param results: contains labels and coordinates predicted by model on the given frame.\n",
    "        :param frame: Frame which has been scored.\n",
    "        :return: Frame with bounding boxes and labels ploted on it.\n",
    "        \"\"\"\n",
    "        n = len(labels)\n",
    "        if n != 0:\n",
    "            x_shape, y_shape = frame.shape[1], frame.shape[0]\n",
    "            for i in range(n):\n",
    "                row = cord[i]\n",
    "                if row[4] >= 0.5:\n",
    "                    x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)\n",
    "                    bgr = (0, 255, 0)\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), bgr, 2)\n",
    "                    cv2.putText(frame, f'Hand: {row[4]}%', (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.9, bgr, 2)\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(cord, frame):\n",
    "\n",
    "    x_shape, y_shape = frame.shape[1], frame.shape[0]\n",
    "\n",
    "    row = cord\n",
    "    if row[4] >= 0.5:\n",
    "        x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)\n",
    "        frame = frame[y1:y2, x1:x2]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(image):\n",
    "    x_shape, y_shape = image.shape[1], image.shape[0]\n",
    "    image_transform = transforms.Compose(\n",
    "            [   \n",
    "                transforms.CenterCrop(max(x_shape,y_shape)),\n",
    "                transforms.Resize(MODEL_IMG_SIZE),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=DATASET_MEANS, std=DATASET_STDS),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image = image_transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hands(preds, crop_image, frame):\n",
    "    x_shape, y_shape = crop_image.shape[1], crop_image.shape[0]\n",
    "    \n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    pred_keypoints = heatmaps_to_coordinates(preds).squeeze(0)\n",
    "\n",
    "    if x_shape > y_shape:\n",
    "        keypoint_tuples = [tuple((np.multiply(i, x_shape)-[0, (x_shape-y_shape)//2]).astype(int)) for i in pred_keypoints]\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        keypoint_tuples = [tuple((np.multiply(i, y_shape)-[(y_shape-x_shape)//2, 0]).astype(int)) for i in pred_keypoints]\n",
    "\n",
    "    \n",
    "    for i in keypoint_tuples:\n",
    "        cv2.circle(crop_image, i, 2, (255,0,0), 2)\n",
    "        \n",
    "    \n",
    "    for segment in JOINT_LIST:\n",
    "        for i in range(len(segment)-1):\n",
    "            crop_image = cv2.line(crop_image, keypoint_tuples[segment[i]], keypoint_tuples[segment[i+1]], (0,128,0), 1, cv2.LINE_AA)\n",
    "    return crop_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hands(crop_coords, image, model):\n",
    "    image_cropped = crop_image(crop_coords, image)\n",
    "    image_tensor = pre_process(image_cropped)\n",
    "    preds = model(image_tensor)\n",
    "    \n",
    "    x_shape, y_shape = image_cropped.shape[1], image_cropped.shape[0]\n",
    "    x1, y1, x2, y2 = int(crop_coords[0]*image.shape[1]), int(crop_coords[1]*image.shape[0]), int(crop_coords[2]*image.shape[1]), int(crop_coords[3]*image.shape[0])\n",
    "\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    pred_keypoints = heatmaps_to_coordinates(preds).squeeze(0)\n",
    "\n",
    "    keypoint_tuples = []\n",
    "    for i in pred_keypoints:\n",
    "\n",
    "        if x_shape > y_shape:\n",
    "            temp_coords = np.multiply(i, x_shape)\n",
    "            temp_coords -= [0,(x_shape-y_shape)/2]\n",
    "            temp_coords += [x1, y1]\n",
    "            temp_coords = tuple(temp_coords.astype(int))\n",
    "            keypoint_tuples.append(temp_coords)\n",
    "\n",
    "        else:\n",
    "            temp_coords = np.multiply(i, y_shape)\n",
    "            temp_coords -= [(y_shape-x_shape)/2, 0]\n",
    "            temp_coords += [x1, y1]\n",
    "            temp_coords = tuple(temp_coords.astype(int))\n",
    "            keypoint_tuples.append(temp_coords)\n",
    "\n",
    "    for i in keypoint_tuples:\n",
    "        cv2.circle(image, i, 2, (255,0,0), 2)\n",
    "    \n",
    "    for segment in JOINT_LIST:\n",
    "        for i in range(len(segment)-1):\n",
    "            cv2.line(image, keypoint_tuples[segment[i]], keypoint_tuples[segment[i+1]], (0,128,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Program Code:\n",
    "Ensure you have run all the above cells before running the below cell. Ensure your webcam is not being used by any other applications. Press 'Q' on keyboard to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "Object_colors = list(np.random.rand(80,3)*255)\n",
    "Object_classes = ['Hand']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Change colour format from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Flip image along y axis\n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    # Set flag to false\n",
    "    image.flags.writeable = False\n",
    "\n",
    "    objs = model(image)\n",
    "\n",
    "    labels, cord_thres = objs.xyxyn[0][:, -1].cpu().numpy(), objs.xyxyn[0][:, :-1].cpu().numpy()\n",
    "\n",
    "    image = plot_boxes(labels, cord_thres, image)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        image = draw_hands(\n",
    "            crop_coords=cord_thres[i],\n",
    "            image=image,\n",
    "            model=unet_model,\n",
    "        )\n",
    "\n",
    "\n",
    "    # Change colour format from RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imshow('Hand Tracking', image)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code in case of any crashes to release resources for the webcam for other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d22ae1c124e37de917c126561a27c788d6dd62390fbbf984c1dcb6dec8f7002b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
